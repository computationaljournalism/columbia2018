{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mongo Basics\n",
    "------------\n",
    "\n",
    "We are going to spend a little time familiarizing you with the basics of how to interact with Mongo. Many of your projects will likely involve Twitter and it is designed to be efficient with tweet-style data. First, as we did last time, let's create a client object to talk to the database. We are going to use the databases of 20k (from The Gateway Pundit) and 260k (that refer to Crisis Actors).  They are hosted on a server we created, following the procedure from last time, having IP address `34.229.68.155`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient(\"mongodb://journalist:secret@34.229.68.155:27017/tweets\")\n",
    "type(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a connection, we specify a database..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db = client[\"tweets\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember in a Mongo database, we have databases that might correspond to different projects. Our project involves tweets associated with the crisis actors story. Inside a database you will have one or more \"collections\" of documents. Each document is made up of JSON objects. We can see the different collections by asking the database to list the `collection_names()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.collection_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have two. Let's start by working with the smaller one for the moment. We'll select the collection `gatewaypundit`. We choose it by referencing like we would a dictionary. We just refer to its name. \n",
    "\n",
    "Again, our database `db` is `tweets` and the collection we want is `gatewaypundit`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pundit = db[\"gatewaypundit\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pundit.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the numbers are consistent. 19k records in the database, one for each tweet. We could have done this all in one line too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pundit = client[\"tweets\"][\"gatewaypundit\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pymongo` interface tries to make things as Python-like as possible. You access databases and the collections they contain by name, as you would elements in a dictionary.\n",
    "\n",
    "So, either way, we now have a reference to the set of tweets that all contain a link to the David Hogg story on The Gateway Pundit. Here is one. Again, it's just Python's instantiation of a JSON string. It is a dictionary made up of built-in data types, lists and other dictionaries. Or, in Mongo-speak, a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pundit.find_one()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Making simple queries**\n",
    "\n",
    "A database not only regularizes access to data for a team, it also provides us with basic search capabilities. Here we are going to show how to pull targeted subsets of data from a Mongo database. We begin with a simple exact match. Here we are looking for tweets that have a `lang` field that is equal to `nl` for Dutch. I'm not sure how that happens but let's see how many there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pundit.find({\"lang\": \"nl\"}).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how many distinct entries our 19K tweets have for language using the command `distinct()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than an exact match of to a string, below we use a regular expression to find a pattern in the `source` field. Here we find sources that refer to \"periscope\". For this, we make use of an operator to specify the documents of interest.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`{\"source\": {\"$regex\":r\"gab\\.ai\"}}`\n",
    "\n",
    "Let's see how many there are..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pundit.find({\"source\": {\"$regex\":r\"gab\\.ai\"}}).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `$regexp` expression is called an **operator**. It is one of many that Mongo makes availabe to express constraints on data to form subsets. Given our experience with Pandas, we might look for things like pattern matching in strings, or various inequalities for numerical data.\n",
    "\n",
    "In the expression below, we form a search for all the tweets with a `retweet_count` equal to 0. The numerical operators include `$eq`, `$gt` and `$lt`  -- equality, greater-than and less-than. Here's what the expression would look like.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`{\"retweet_count\":{\"$gt\":2000}}`\n",
    "\n",
    "And let's form a count..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pundit.find({\"retweet_count\":{\"$eq\":0}}).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other operators include `$exists`, which simply tests if some part of the document (the tweet) is present or not. Why is this useful? Twitter only includes the `retweeted_status` field if the tweet is a retweet. If it is an original post, that field won't be part of the tweet. \n",
    "\n",
    "Here we see how many tweets are not retweets in our collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pundit.find({\"retweeted_status\":{\"$exists\":False}}).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pundit.find({\"retweeted_status\":{\"$exists\":True}}).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a [complete list of operators you can use with Mongo](https://docs.mongodb.com/manual/reference/operator/query/)\n",
    "\n",
    "If we want to have several conditions in our match, we can just add more items to our dictionary. Here we take all the tweets that are retweets and keep just the ones that start with \"RT \". \n",
    "\n",
    "The \"documents\" we've been making to perform searches (create subsets) of data are called **filters**. They take on the structure of the original data. Remember? Our tweets have keys like `retweeted_status` or `text` and so our filters! Ah you can't chop down a symmetry!\n",
    "\n",
    "Here we ask for the count of our tweets that are retweets and their language is Dutch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pundit.find({\"lang\":\"nl\", \"retweeted_status\":{\"$exists\":True}}).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than just counting, we can iterate through the set to display our results. Here we search for tweets with a retweet count of 0 and we only keep the fields `text` and `user.screen_name` (the \".\" is how we index into an embedded document, `screen_name` being a key to the `user` dictionary of the tweet). \n",
    "\n",
    "The notation in the second dictionary assigns a value of `True` to a key if you want to keep the variable with that name and it assigns a `False` otherwise. (You will also see 1 and 0 instead -- remember `True` reduces to 1 and `False` to 0.) This is called **a projection**. The first two arguments to `find()` are `filter=` and `projection=`.  In the code below, we are leaving out Mongo's `_id` variable as it's an internal Mongo index that's really ugly to look at. \n",
    "\n",
    "**Ah and notice that the projection is also a document, just like filter is, matching our data. **\n",
    "\n",
    "In the code below we have also added `.limit(10)` to the back of the command so that we only get 10 items from the database. We'll show you how to sort the data first and get the 10 with the largest retweet counts or some other condition shortly. \n",
    "\n",
    "But for now we are printing out the JSON that's returned from the database and then a more formatted printout of the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in pundit.find({\"retweet_count\":{\"$eq\":0}},{\"text\":True,\"user.screen_name\":True,\"_id\":False}).limit(10):\n",
    "    # the dictionary from Mongo\n",
    "    print(tweet)\n",
    "    print(\"---\")\n",
    "    # printing out some data from the dictionary\n",
    "    print(tweet[\"user\"][\"screen_name\"])\n",
    "    print(tweet[\"text\"])\n",
    "    print(\"===\"*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK so above the `---` we have the dictionary object returned by Mongo. It has only the two items we wanted -- the text of the tweet and the user's screen_name. Just to show it's a dictionary, we then selected the `screen_name` and `text` separately. OK?\n",
    "\n",
    "One of the most retweeted updates in our database came from `@thegatewaypundit` himself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "\n",
    "<blockquote class=\"twitter-tweet\" data-lang=\"en\"><p lang=\"en\" dir=\"ltr\">EXPOSED: School Shooting Surviver Turned Activist David Hogg&#39;s Father in FBI, Appears To Have Been Coached On Anti-Trump Lines [VIDEO] <a href=\"https://t.co/z2T0LgmyQ9\">https://t.co/z2T0LgmyQ9</a></p>&mdash; Jim Hoft (@gatewaypundit) <a href=\"https://twitter.com/gatewaypundit/status/965720917883146248?ref_src=twsrc%5Etfw\">February 19, 2018</a></blockquote>\n",
    "<script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how often it was a retweet among our collection of tweets. As we mentioned, we can decide if a tweet is a retweet by looking in the tweet object for the `retweeted_status` field.  When it exists, your tweet is a retweet. \n",
    "\n",
    "And if it exists, the field holds the original tweet being retweeted. So, you can look inside `retweeted_status` for the `id_str` (a string version of the tweet id -- why have both a number and a string?) of the original tweet. In our case, we are looking for an `id_str` with value \"965720917883146248\". We could also use `$eq` with the numerical value of the id. (Why have a string and a numerical version of the id?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pundit.find({\"retweeted_status.id_str\":\"965720917883146248\"}).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see who retweeted the pundit. We can use the same filter document and won't restrict the output with  projection -- we've left the second argument to `find()` off. Here's a simple printout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in pundit.find({\"retweeted_status.id_str\":\"965720917883146248\"},{\"user.screen_name\":True,\"user.followers_count\":True,\"_id\":False}).limit(10):\n",
    "    print(tweet[\"user\"][\"screen_name\"], tweet[\"user\"][\"followers_count\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The thing we want to do next is simply sort by followers. Who among those that retweeted the pundit has the most followers? We can sort the results of our search for retweeters by `followers_count`, here from largest to smallest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import ASCENDING, DESCENDING\n",
    "\n",
    "for tweet in pundit.find({\"retweeted_status.id_str\":\"965720917883146248\"}).sort(\"user.followers_count\",DESCENDING).limit(10):\n",
    "        print(tweet[\"user\"][\"screen_name\"], tweet[\"user\"][\"followers_count\"])\n",
    "        print(tweet[\"user\"][\"description\"])\n",
    "        print(\"---\"*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here from smallest to largest, adding the number of friends as well to the output. Notice that many don't have proper descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import ASCENDING, DESCENDING\n",
    "\n",
    "for tweet in pundit.find({\"retweeted_status.id_str\":\"965720917883146248\"}).sort(\"user.followers_count\",ASCENDING).limit(10):\n",
    "        print(tweet[\"user\"][\"screen_name\"], tweet[\"user\"][\"followers_count\"], tweet[\"user\"][\"friends_count\"])\n",
    "        print(tweet[\"user\"][\"description\"])\n",
    "        print(\"---\"*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, Python's syntax for `sort()` is different than that used by Mongo. Mongo again deals in a document to specify a key and a direction. But pymongo wants a list of keyname-direction pairs. Alas.\n",
    "\n",
    "Now, let's take the retweet times for these retweets and see what they look like. We will load a data frame from our Mongo search using the `from_records()` method we saw briefly last time, and again use our `Grouper`. Here we kept just the `created_at` field and you can see why we might want to be a little sparing about the data we bring over from Mongo. We don't need a hundred fields for this task, so we can leave them behind.\n",
    "\n",
    "Remember that we have seen a couple ways to manufacture a dataframe. One involves a dictionary where the keys are column names and the values are lists representing columns of data. We can also have a list of lists, where the inner lists are the data for each row and the outer list forms the table, holding an ordered set of rows. The \"records\" formulation uses a list of dictionaries, where each dictionary in the list has keys that are column names and represents a single row. \n",
    "\n",
    "You can see the interplay between CSV and this \"records\" format is seen in [Mr. Data Converter](https://shancarter.github.io/mr-data-converter/) where if you choose \"use sample\" you will see what we mean. Rows becoming dictionaries and tables becoming lists of dictionaries.\n",
    "\n",
    "That's what we're doing here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas  import DataFrame, Grouper, to_datetime\n",
    "\n",
    "pundit_retweets = DataFrame.from_records(pundit.find({\"retweeted_status.id_str\":'965720917883146248'},{\"created_at\":True,\"_id\":False,\"id\":True}))\n",
    "pundit_retweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next few lines of code we've seen frequently. We turn the string representation of time into a Python datetime object and then use it as the index for our data frame. We can then apply the `TimeGrouper` (because it wants to group things by a time-based index) and look at counts per 15 minutes. The last printout is our final counts data frame that tells us how often the Trump Jr. tweet was retweeted in each 15 minute epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pundit_retweets[\"stamp\"] = to_datetime(pundit_retweets[\"created_at\"],format='%a %b %d %H:%M:%S +0000 %Y')\n",
    "pundit_retweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = pundit_retweets.groupby(Grouper(key=\"stamp\",freq='30min')).agg({\"id\":\"count\"}).rename(columns={\"id\":\"count\"})\n",
    "counts.reset_index(inplace=True)\n",
    "\n",
    "counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And sure, let's plot it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.plotly import iplot, sign_in\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "sign_in(\"cocteautt\",\"8YLww0QuMPVQ46meAMaq\")\n",
    "\n",
    "myplot_parts = [go.Scatter(x=counts[\"stamp\"],y=counts[\"count\"],mode=\"line\")]\n",
    "mylayout = go.Layout(autosize=False, width=1000,height=500)\n",
    "myfigure = go.Figure(data = myplot_parts, layout = mylayout)\n",
    "iplot(myfigure,filename=\"crisis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A processing pipeline**\n",
    "\n",
    "So far we have just pulled subsets of data from Mongo and created a dataframe for further processing. This might be wasteful. The database is a powerful aggregation engine as well, meaning we can do some procesing in an optimized environment before pulling data into Python. The pipeline is accessed via a method called `aggregate()`. \n",
    "\n",
    "In this method you specify some aggregtors, operators if  you will. There are operators to `$match`, limiting our data to a subset, to `$group` the subset by one or more variables so you can perform summaries like counting or summing, and finally to `$project` the data, keeping only those terms we want.  Each stage in the pipeline transforms documents. [Here is a list of the operators you can use at each stage.](https://docs.mongodb.com/manual/reference/operator/aggregation/#aggregation-pipeline-operator-reference) \n",
    "\n",
    "<img src=https://docs.mongodb.com/manual/_images/aggregation-pipeline.bakedsvg.svg width=800>\n",
    "\n",
    "Below we use the `$group` operator to group by the name of a person being retweeted. So we look into the `retweeted_status` field, if it's there, and pull out the id of the tweet that was retweeted. That groups the data. Then, for each group we pull the person's name who  tweeted said tweet and then sum up the number of times it was retweeted. \n",
    "\n",
    "The synatax below is that `aggregate()` takes a list of operators, steps in the pipeline. They tend to be (as in the image above) things like `$match` to find tweets that match a given criterion (like `find()`) and `$project` that basically renames columns. We'll also see `$sort` for, well, sorting the output. This will look similar to what we are doing in `find()` but `find()` is pretty limited. The `aggregate()` pipeline lets you do more stuff and more  efficiently in the database. \n",
    "\n",
    "Our first pipelines will be lists with just one element -- `$group`. It's a boring pipeline but we have already seen in Pandas how powerful grouping and counting can be.\n",
    "\n",
    "In forming the expression below, we have operators like `$group` that are specifying actions in the pipeline. We also have operators that take actions on groups. These include `$sum` and `$first`. The former adds up data in the group and the latter returns the first value in the group. In our case, all the tweets in a group have the same originator and so the first and last and all the screen names should be the same. Consult the operator reference for the other things you can do. \n",
    "\n",
    "Oh and for `$group`, you are first speciying the variable(s) to group by with a dictionary key of `_id`. You then name columns in your new data set as keys in the rest of the document, with dictionaries for how to compute them. When you have a name from your original data (coming from Mongo) you add a `$`. If you leave it off, then Mongo will assume you are specifying a constant value and not taking one from the database.\n",
    "\n",
    "The `aggregate()` method gives us back something we can iterate over (in a loop, say) by using the `.next()` method. We usually don't touch that because a for-loop would call it for us. But here we just want to see one entry. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pundit.aggregate([{\"$group\":{\"_id\":\"$retweeted_status.id_str\",\"name\":{\"$first\":\"$retweeted_status.user.screen_name\"},\"text\":{\"$first\":\"$retweeted_status.text\"},\"counts\":{\"$sum\":1}}}]).next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have one tweet and its retweet count. Let's  do this for all the tweets in our data set. Instead of using one record, we'll pass the result of `.aggregate()` to the `from_records` method of a data frame and get back a table of all the tweets and their retweet counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retweet_counts = DataFrame.from_records(\n",
    "                   pundit.aggregate([{\"$group\":{\"_id\":\"$retweeted_status.id_str\",\n",
    "                                                \"name\":{\"$first\":\"$retweeted_status.user.screen_name\"},\"text\":{\"$first\":\"$retweeted_status.text\"},\"counts\":{\"$sum\":1}}}\n",
    "                                    ]))\n",
    "retweet_counts.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, maybe we want to look at the top of the table, the big values, instead. Let's use what we recall from pandas to do that. Once we have `df` we can just sort..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retweet_counts.sort_values(\"counts\",ascending=False,inplace=True)\n",
    "\n",
    "# reset the index because the sorting has mixed things up\n",
    "retweet_counts.reset_index(inplace=True,drop=True)\n",
    "retweet_counts.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline we fed to `aggregate` is a list. It is a list of things to do to the data in order. Here we add the sorting of our tweets into the pipeline rather than do it in pandas. It means adding the operator `$sort` to the mix. `$group` is the first element in the list and `$sort` is the second.  The `-1` means sort `counts` in decreasing order. Now we have a 2-element pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retweet_counts = DataFrame.from_records(\n",
    "                      pundit.aggregate([{\"$group\":{\"_id\":\"$retweeted_status.id_str\",\n",
    "                                                   \"name\":{\"$first\":\"$retweeted_status.user.screen_name\"},\"text\":{\"$first\":\"$retweeted_status.text\"},\"counts\":{\"$sum\":1}}},\n",
    "                                       {\"$sort\":{\"counts\":-1}}\n",
    "                                      ]))\n",
    "retweet_counts.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might reasonably ask why do we need all these functions if Pandas can do things for us. Well, we appeal to a database when we cannot hold all our data in a data frame. In this case, too, the data are somewhat irregular in the form of the data changes.\n",
    "\n",
    "Finally, let's look at more of the retweets and see which happened when. We use the data frame we just made and pull out the id's of each popular tweet. We cycle through just as we did above for a single tweet, adding more lines to our plot. Here we go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas  import DataFrame, TimeGrouper, to_datetime\n",
    "\n",
    "myplot_parts = []\n",
    "\n",
    "for tweetid in retweet_counts[\"_id\"][1:20]:\n",
    "    \n",
    "    tmp = DataFrame.from_records(pundit.find({\"retweeted_status.id_str\":tweetid},{\"created_at\":True,\"id\":True}))\n",
    "    tmp[\"stamp\"] = to_datetime(tmp[\"created_at\"],format='%a %b %d %H:%M:%S +0000 %Y')\n",
    "\n",
    "    counts = tmp.groupby(Grouper(key=\"stamp\",freq='60min')).agg({\"id\":\"count\"}).rename(columns={\"id\":\"count\"})\n",
    "    counts.reset_index(inplace=True)\n",
    "\n",
    "    myplot_parts.append(go.Scatter(x=counts[\"stamp\"],y=counts[\"count\"],mode=\"line\",name=tweetid))\n",
    "\n",
    "mylayout = go.Layout(autosize=False, width=1000,height=500)\n",
    "myfigure = go.Figure(data = myplot_parts, layout = mylayout)\n",
    "iplot(myfigure,filename=\"crisis\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's repeat the same code, but look at frequent tweeters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_tweeters = DataFrame.from_records(\n",
    "                          pundit.aggregate([{\"$group\":{\"_id\":\"$user.screen_name\",\"counts\":{\"$sum\":1}}},\n",
    "                          {\"$sort\":{\"counts\":-1}}\n",
    "                         ]))\n",
    "frequent_tweeters.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pandas  import DataFrame, TimeGrouper, to_datetime\n",
    "\n",
    "myplot_parts = []\n",
    "\n",
    "for who in frequent_tweeters[\"_id\"][0:20]:\n",
    "    \n",
    "    tmp = DataFrame.from_records(pundit.find({\"user.screen_name\":who},{\"created_at\":True,\"id\":True}))\n",
    "    tmp[\"stamp\"] = to_datetime(tmp[\"created_at\"],format='%a %b %d %H:%M:%S +0000 %Y')\n",
    "\n",
    "    counts = tmp.groupby(Grouper(key=\"stamp\",freq='60min')).agg({\"id\":\"count\"}).rename(columns={\"id\":\"count\"})\n",
    "    counts.reset_index(inplace=True)\n",
    "\n",
    "    myplot_parts.append(go.Scatter(x=counts[\"stamp\"],y=counts[\"count\"],mode=\"line\",name=who))\n",
    "\n",
    "mylayout = go.Layout(autosize=False, width=1000,height=500)\n",
    "myfigure = go.Figure(data = myplot_parts, layout = mylayout)\n",
    "iplot(myfigure,filename=\"crisis\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's look at some of the frequent tweeter's tweets. Here is `@a_troody`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import set_option\n",
    "\n",
    "set_option(\"display.max_colwidth\",280)\n",
    "set_option(\"display.max_rows\",100)\n",
    "\n",
    "# pull the tweets from the database\n",
    "atr = DataFrame.from_records(\n",
    "        pundit.find({\"user.screen_name\":\"a_troody\"},{\"created_at\":True,\"retweeted_status.user.screen_name\":True,\"text\":True}))\n",
    "\n",
    "# add a timestamp\n",
    "atr[\"stamp\"] = to_datetime(tx[\"created_at\"],format='%a %b %d %H:%M:%S +0000 %Y')\n",
    "\n",
    "# sort them in time\n",
    "atr.sort_values(\"stamp\").head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pull all the tweets from people with more than 10000 followers. For each tweet from these people, we keep the tweet id, their screen name and the retweet count of the given tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = DataFrame.from_records(pundit.find({\"user.followers_count\":{\"$gt\":10000}},{\"_id\":False,\"user.screen_name\":True,\"id\":True,\"retweet_count\":True}).limit(100))\n",
    "tmp.head()                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with `find()` here is that the data frame now holds an embedded document, a dictionary, that represents the part of the `user` data we have decided to keep. We don't want a dictionary, we want a string representing the screen name. \n",
    "\n",
    "To get around this, we use `aggregate()` and `$project` the document, reshaping  it to just include the id of the tweet and the user's screen_name, but getting rid of the extra dictionary. Again, the pipeline is represented as a list of dictionaries where the keys match operations at each stage. Here we use `$match` to form a subset and `$project` to flatten things out.\n",
    "\n",
    "Here's an example and then how you can use it to create a data frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And in data frame form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biggies = DataFrame.from_records(pundit.aggregate([{\"$match\":{\"user.followers_count\":{\"$gt\":10000}}},\n",
    "                                                   {\"$project\":{\"id\":True,\"screen_name\":\"$user.screen_name\",\"retweet_count\":True,\"_id\":False}}]))\n",
    "biggies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we might form another pair, this time grouping all the tweets from the high-follower count people and see how many times their tweets with the gateway pundit article link have been retweeted. The first two make sense being the guy behind The Gateway Pundit and the reporter on the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataFrame.from_records(pundit.aggregate([{\"$match\":{\"user.followers_count\":{\"$gt\":10000}}},\n",
    "                                              {\"$group\":{\"_id\":\"$user.screen_name\",\"counts\":{\"$sum\":\"$retweet_count\"}}}]))\n",
    "df.sort_values(\"counts\",ascending=False,inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very fast introduction to a powerful aspect of Mongo. Here is a complete [description of aggregation](https://docs.mongodb.com/manual/meta/aggregation-quick-reference/#aggregation-expressions)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
