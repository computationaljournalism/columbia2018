{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[The introductory material for this notebook was prepared by Suman Deb Roy.]\n",
    "\n",
    "### Recommender Systems\n",
    "\n",
    "We live in a world where information production has multiplied exponentially. Ninety percent of the world's digital data was created in the last two years! What happens now? \n",
    "\n",
    "<img src=\"https://gigaom.com/wp-content/uploads/sites/1/2011/02/3047760160_f869b55dda_z.png\" style=\"width: 55%;\"/>\n",
    "<br>\n",
    "\n",
    "**Endless choice**. Never have we have we ever had so much choice in terms of news, information, music and entertainment. The \"democratisation\" of production means its easier than ever for people to create content. Sadly, this also means we are receiving more than we can consume -- it's even hard to know the full extent of the information being produced daily, its size and shape and inflection.\n",
    "\n",
    "In media circles, people refer to this moment of over-production as \"Peak Content,\" [the point at which this glut of things to read, watch and listen to becomes completely unsustainable](https://www.themediabriefing.com/article/peak-content-the-collapse-of-the-attention-economy). \n",
    "\n",
    "**Attention**. In that sense, everything is competing for your attention, starting from museums to social networks to TV shows to your gaming console. The power of a connected world is obvious - we disseminate information, not just from consolidated systems but from the edges of the network. \n",
    "\n",
    "To prevent information overload, traditional curation has been the solution for a long time. But at some point, someone figured out that even curation can't keep up with this information age. People are different not in their fondness for the common or popular information, but in their affinity for very specific sections of the \"long tail.\" (\"The theory of the Long Tail is that our culture and economy is increasingly shifting away from a focus on a relatively small number of 'hits' -- mainstream products and markets -- at the head of the demand curve and toward a huge number of niches in the tail. [See Chris Anderson on the topic,](http://www.longtail.com/the_long_tail/faq/) and loop in Dave Carroll's talk Monday evening.) In other words, specific sections of the long tail can attract your attention more than the usual boring mainstream information.\n",
    "\n",
    "And thus began the age of internet information systems with the mission to give you **exactly** what you want. Yahoo! tried to do this using a directory structure that was larger selection of topics than newspaper beats. \n",
    ">\"In April 1994 David Filo and Jerry Yang created the Yahoo! Directory as a collection of their favorite web pages. As their number of links grew they had to reorganize and become a searchable directory.\" [From Search Engine History.](http://www.searchenginehistory.com/)\n",
    "\n",
    "And eventually, Google figured out technology to rank web sites satisfying your \"query\", a keyword or sentence. [See Brin and Page's \"In this paper we present Google...\"](http://infolab.stanford.edu/~backrub/google.html)\n",
    "\n",
    "**Can I make a recommendation?**. Search engines rose to prominence in the mid-1990s promising to help users find almost anything on the Web. But there was one problem - *you still had to know what to search for.* This is what recommendation tries to solve. Given an existing data trail, the system is supposed to tell you how you might interact with the system next. For Google, this is recommending you a similar phrase to search. For Amazon, its recommending you a similar product to buy. For [Facebook, making sensible recommendations is their ultimate challenge](https://code.facebook.com/posts/861999383875667/recommending-items-to-more-than-a-billion-people). \n",
    "\n",
    "> There is a growing convergence of search and recommendations. In fact, recommender algorithms tie together three key themes that form a kind of technical oligopoly that dominates the market for your attention: (1) search engines, (2) recommendation systems and (3) smart push notifications. \n",
    "\n",
    "As you can see, it seems like humans have become comfortable offloading as much of their decision making to software as possible, in many cases (like shopping, travel) at least. So recommendation systems became the norm of internet websites. Thus arose an entire field of computing around recommender systems, and there is an [entire ACM conference dedicated](https://recsys.acm.org/) to it. I always find it fascinating how much Computer Science as an academic field is driven by what the industry needs. \n",
    "\n",
    "Again, we draw your attention to Kosinski's work and Dave Carroll's talk about how this consolidation can go very wrong.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this session, we will consider some key aspects of recommender systems, play around with data and explore the features that an optimal news recommendation system should possess! We will cover the following things:\n",
    "\n",
    "1. Examples/Impact of recommendations \n",
    "2. Ways to recommend\n",
    "3. Is news recommendation a unique problem\n",
    "4. Make your own recommendation engine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Why this item isn't the most important - it's the next one\n",
    "\n",
    "The most imporant article on your website is the second article that someone reads. If that next article is also as good as the current one, then you’ve established something meaningful. Thats how the user will start trusting you. \n",
    "\n",
    "\n",
    "- **Site Walk**. For many original content sites, only 10% of visits are organically to the homepage of content sites. (This is certainly not the way it used to be when the web was young and shiny! Why is it true now?) The vast majority of people spend time on the content pages (which may or may not be linked on homepage). The journey from one article to another is important. This means the power of internal suggestion tools in a website is more critical than anything else. \n",
    "\n",
    "\n",
    "- **High Engagement**. The ability to make relevant suggestions is at the core of any content business. Looking at Netflix, their recommender system is used on most screens of the product beyond the homepage, and on average influences about 80% of the hours of content they stream.  (Curious where the other 20% comes from?) \n",
    "\n",
    "\n",
    "- **\\$\\$ utility**. The combined effect of personalization and recommendations is to increase engagement levels and reduce churn significantly. Is the effect of recommendations measurable? Yes - this saves Netflix more than \\$1B per year. You can read more here: [The Netflix Recommender System: Algorithms, Business Value, and Innovation](http://dl.acm.org/citation.cfm?id=2843948)\n",
    "\n",
    "\n",
    "- **Mix with curation**. In many of the world's most beloved services, recommendations play an invisible yet unrelenting role that is not only critical in keeping consumers happy, but also necessary in scaling the underlying information systems. Here's [how Spotify chooses what makes it onto your Discover Weekly playlist](http://www.wired.co.uk/article/tastemakers-spotify-edward-newett)\n",
    "\n",
    "\n",
    "- **Personal/Privacy**. Also, recall that recommendations are customized to YOU and so use a lot of they collect about you -- including, but not limited to, your usage history of the service -- and thus, can be deeply personal.\n",
    "<br><br>\n",
    "<img src = \"https://imgs.xkcd.com/comics/pandora.png\">\n",
    "<br><br>\n",
    "Want to know *everything* that you've done in Google lately. [Here you go](https://myactivity.google.com/myactivity). \n",
    "\n",
    "\n",
    "- One <font color='red'>warning</font>: Recommendations re-order the set of all possible answers to a question by relevance (creating a list of books or web sites or movies or songs). If you interact with that list by picking the TOP answer - then you have created a kind of algorithmic personal assistant. \n",
    "<br><br>\n",
    "This is how Siri or Alexa or Google Home decides on the ONE answer it gives you - it searches the world, aggregates all candidate (relevant) stories, orders them by RECOMMENDED STORIES and then picks the first one! Of course, other signals might be involved (like the device you are using) but that core process needs to be run. And [when this recommendation list goes wrong, ONE true answers turn nasty](http://searchengineland.com/googles-one-true-answer-problem-featured-snippets-270549)\n",
    "\n",
    "\n",
    "- Ok one more <font color='red'>warning</font>: What happens when we recommend \"similar\" things? We end up in a bubble. Trusting recommendations blindly is what people do, without knowing how its made or the internal biases it has. That in turn leads to [rating bubbles.](http://sloanreview.mit.edu/article/the-problem-with-online-ratings-2/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to recommend the next one?\n",
    "\n",
    "- **Your data format/content item structure matters.** Almost every company uses some kind of machine learning with whatever format of data their product generates to recommend items from their inventory. Here are some links on how these individual products \"do\" recommendation. Let's each pick one and skim the techniques and then talk about it.\n",
    "\n",
    " 1.  Pins - [PInterest](https://medium.com/@Pinterest_Engineering/pinnability-machine-learning-in-the-home-feed-64be2074bf60#.6906fsamn)\n",
    " 2. Videos -  [YouTube](https://pdfs.semanticscholar.org/e7d5/3f538f5239739d1f943c81d17e4a167c65c6.pdf)\n",
    " 3. Communities - [Reddit](http://jaybaxter.net/redditrecommender.pdf)\n",
    " 4. Music - [Pandora](https://arstechnica.com/tech-policy/2011/01/digging-into-pandoras-music-genome-with-musicologist-nolan-gasser/)\n",
    " 5. Rentals - [AirBnb](https://www.forbes.com/sites/ellenhuet/2015/06/05/how-airbnb-uses-big-data-and-machine-learning-to-guide-hosts-to-the-perfect-price/#30975ecf6d49)\n",
    " 6. Job profiles - [LinkedIn](https://blog.linkedin.com/2011/03/02/linkedin-products-you-may-like)\n",
    " 7. Search - [Google Now](http://dl.acm.org/citation.cfm?id=2959192)\n",
    " 8. Social NewsFeeds - [Facebook](http://science.sciencemag.org/content/348/6239/1130)\n",
    " 9. Travel/Flights - [Expedia](https://techblog.expedia.com/2016/03/07/how-expedia-finds-flights-a-detailed-view/)\n",
    " 10. Your shopping cart - [Amazon](http://www.cs.umd.edu/~samir/498/Amazon-Recommendations.pdf)\n",
    "\n",
    "\n",
    "- **Data generating products:** The beauty of designing a system is that you can nudge the users to CREATE data that assists future recommendations. Everything from \"Like\" buttons to how long they spend on a particular article can be used to model their affinity and predispositions. Designers spend a lot of time doing this, so interactions become feedback.\n",
    "\n",
    "\n",
    "- **Filtering:** Filtering the right items can make your recommendations much more useful and improve user experience.\n",
    " 1. Something that's not an item: Don't recommend videos in a articles feed, maybe make a new landing page. \n",
    " 2. Something that is stale/not applicable anymore: Don't recirculate old relevance.  \n",
    " 3. Something the user has already seen: Been there, done that. \n",
    " 4. Something that can offend an user: Careful. \n",
    " 5. Something that miscalculates user-topic granularity: Don't recommend Android news to Apple fans, although both are in tech.\n",
    "\n",
    " 6. *....  your recommendations for filtering here*\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notable Algorithms and Mechanisms\n",
    "\n",
    "There are two widely used ideas for recommendation. The limiting factor, as you've guessed already, is the data that you are capturing. Here's why:\n",
    "\n",
    "- **Topic level modeling.** The first way is super simple: observe the **semantics** of content that the user is reading, and recommend her something similar. In other words, recommend an user reading about Messi more stories about Messi or Soccer or Sports etc. \n",
    "\n",
    "- **User co-interaction modeling**. The second method models users by their similar actions in a content space. For example, imagine users P1,P2 and P3 reads articles X,Y,Z,P,Q. Now assume a new user P4 reads article X,Y,Z and P. There is a high chance that user P4 can be recommended article Q because other users with the same pattern have read it. *If you eyebrows are raised thats totally valid*\n",
    "\n",
    "So what are techniques formally called: \n",
    "\n",
    "| Technique | Basic Idea | Limitations |\n",
    "| ------ | ----------- |\n",
    "|[Collaborative Filtering](https://en.wikipedia.org/wiki/Collaborative_filtering)   |  If a person L has the same opinion as person S on an issue (or issues), L is more likely to have S's opinion on a different issue than that of a randomly chosen person. | [Cold Start](https://en.wikipedia.org/wiki/Cold_start)|\n",
    "|[Semantic Analysis](https://en.wikipedia.org/wiki/Semantic_similarity) | If we can find the category of content a person likes, we can suggest him similar items from related categories.  | Learning new topics. | \n",
    "|[Hybrid](http://techblog.netflix.com/2012/04/netflix-recommendations-beyond-5-stars.html)   | Combine previous two approaches | Balance is tough. |\n",
    "|[A/B Testing ](http://blog.yhat.com/posts/the-beer-bandit.html)   | Start with random choice, improve probability of recommending with click rate | Initial votes matter more.\n",
    "\n",
    "Both topic and user co-interaction modeling have a common drawback: The **Rich get Richer** Effect. \n",
    "Two great related reads: \n",
    "1. [How recommenders (among other things) lead to the downfall of Blockbuster](http://pubsonline.informs.org/doi/abs/10.1287/mnsc.1080.0974), \n",
    "2. [Analyzing the Youtube video recommendation and how it affects video views](https://www.cl.cam.ac.uk/~jac22/camhor/mia_imc07_sumitted.pdf) \n",
    "\n",
    "\n",
    "Hybrids are incredibly powerful when tuned properly, and sometimes can [transform products](https://www.nytimes.com/2014/03/07/business/media/the-sweet-streaming-sound-of-data.html) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A word on Metrics\n",
    "    \n",
    "Clearly none of these models will work unless we can assign a score to each and every item (whether its a news article or song or a movie). How you design these metrics or scores plays a huge role in what gets recommended. \n",
    "\n",
    "<img src = \"https://imgs.xkcd.com/comics/star_ratings.png\">\n",
    "\n",
    "*<u>How <font color= red> NOT </font>to score a Rating!! </u>* But what should we do? Here are some examples. Suggest more!\n",
    "\n",
    "- Score = (total_positive_ratings - total_negative_ratings)\n",
    "\n",
    "- Score = (total_positive_ratings / all_ratings)\n",
    "\n",
    "- *Your exercise: Find our the optimal metric by which ratings should be score*\n",
    "\n",
    "\n",
    "\n",
    "Make sure in an attempt to do things like [reverse engineering Hollywood](https://www.theatlantic.com/technology/archive/2014/01/how-netflix-reverse-engineered-hollywood/282679/), we don't imbue a ratings system with bias. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issues in News Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Surprise: You only want to scroll to find the next interesting thing. \n",
    "\n",
    "\n",
    "- The Long Tail: Enjoyment of a product is much less dictated by the median experience, but instead by the outliers. The barrier to this long tail market is contextual knowledge, specific to you and your interests at this moment in time.\n",
    "\n",
    "\n",
    "- Editors vs. Algorithms: Battle of the Ages. \n",
    "\n",
    "\n",
    "- Segment Recommendation vs. Personalization: is your cohort size =1 ? \n",
    "\n",
    "\n",
    "- Filter Bubble vs. Engagement: The reality is [people REALIZE the consequences with personalized recommendations, but they still want them](http://digitalnewsreport.org/essays/2016/people-want-personalised-recommendations/)\n",
    "\n",
    "\n",
    "- Diversity vs. Irrelevance: Surprise vs. why are you showing me this? \n",
    "\n",
    "\n",
    "- Cold start: I don't know enough about the user to recommend anything \n",
    "\n",
    "\n",
    "- Serendipity: or giving the appearance of serendipitious information discovery. \n",
    "\n",
    "\n",
    "- Robustness: Gaming a recommender system or resistence to [media hacking](https://render.betaworks.com/media-hacking-3b1e350d619c#.njy9cij35). \n",
    "\n",
    "\n",
    "- Trust: A recommender explaining itself! (*Because you watched the matrix*)\n",
    "\n",
    "\n",
    "- Privacy: *[you might also like.. privacy risks in collaborative filtering](https://www.cs.utexas.edu/~shmat/shmat_oak11ymal.pdf)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ranking**\n",
    "\n",
    "At the core of most information systems you encounter on the Web, from search engines to social network newsfeeds to shopping recommendations, is the concept of *Ranking*. We saw that a recommendation engine is producing a ranking as well. This is a common need, and the idea is simple. You could give the user all the content in the world, but clearly he/she does not have time or capacity to consume that. So you have to select what to show *first*. And thus, you have to rank all the content by some parameter, depending on what sort of data features your products have. \n",
    "\n",
    "Try searching something on Google:\n",
    "\n",
    "<img src = \"http://i.imgur.com/thQGPdj.gif\">\n",
    "\n",
    "Notice the footer mentioning the incredibly large number of results the query retrived. Clearly, if Google showed you 10 of those links at random on the front page, it wouldn't be the product it is today. So it has to somehow figure which 10 stories are most relevant or which 10 stories *you are most likely to click* on the front page.\n",
    "\n",
    "As an aside, search results were not always lists. Early on, Altavista had \"live topics\" that allowed for a graph based display of content, a \"dynamic categorisation of results\". [Read more here](http://www.ariadne.ac.uk/issue9/search-engines).\n",
    "\n",
    "![live](http://www.ariadne.ac.uk/images/issue9-search-engines/lt2.gif)\n",
    "\n",
    "Anyway...\n",
    "\n",
    "- The enormous power of these ranking systems come from the intelligence of their algorithms. Think about the Facebook newsfeed: What if it always showed you photos of friends you don't care about? Think of Netflix: What if it recommended movies you didn't care about? Think about Amazon: What if its recommended \"next buy\" is completely different than what you are browsing, and not in a compelling way? Information systems are powerful because there is a core technology that is ranking. \n",
    "\n",
    "- And if that algorithm works, it can [topple empires](http://infolab.stanford.edu/pub/papers/google.pdf) \n",
    "\n",
    "\n",
    "- Research indicates that [91% of searchers do not go past page 1 of the Google search results](https://www.utwente.nl/nl/bms/cw/bestanden/Using%20the%20Internet-%20Skill%20related%20problems.pdf), and  50% do not go past the first 3 results on page 1. Imagine [the competiton to get in that top-3](https://en.wikipedia.org/wiki/Search_engine_optimization). \n",
    "\n",
    "\n",
    "- Google did a lot of research on this. They found people will actually *rewrite* a search query phrase rather than go into the second page. \n",
    "\n",
    "\n",
    "- And as you have seen in 2016, stories that appears on newsfeeds can reshape realities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to build a recommendation engine using the digg links. Once the user enters a link, your goal is to build a recommendation engine that suggests the next link and prioritizes some feature in doing so. For example, the next recommended link can be ranked based on: \n",
    "\n",
    " 1. The top publishers\n",
    " 2. The comment score of articles\n",
    " 3. Similar words in the content\n",
    " 4. Similar topics\n",
    " 5. The number of likes the article got\n",
    " 6. Is it fresh (timestamp)?\n",
    " 7. The number of times the article was shared \n",
    " 8. *You decide*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**A very simple topic link recommender**\n",
    "\n",
    "Let's map out a simple strategy for making recommendations. Fun!\n",
    "- Ask user for a topic. \n",
    "- Recommend 3 links, relevant to topic and \"recent\" \n",
    "- *you can keep stacking rules here*\n",
    "\n",
    "### Case Studies - When Recommendation becomes News\n",
    "\n",
    "1. [The Hollywood Reporter](https://medium.com/@wfhorn/keep-reading-the-story-of-a-nascent-recommendation-engine-fe84a81248c9#.vzs3rtn8w)\n",
    "2. [The NY Times](https://open.blogs.nytimes.com/2015/08/11/building-the-next-new-york-times-recommendation-engine/)\n",
    "3. [Clavis - WashingtonPost](http://knightlab.northwestern.edu/2015/06/03/how-the-washington-posts-clavis-tool-helps-to-make-news-personal/)\n",
    "4. [Apple News](https://www.theguardian.com/technology/2015/jun/16/apple-news-app-editors-curation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=https://github.com/computationaljournalism/columbia2018/raw/master/images/maxresdefault.jpg width=500>\n",
    "    \n",
    "\n",
    "### Collaborative filtering\n",
    "\n",
    "Now we are going to lay the groundwork for a cluster of computational tools that are riled under the names \"statistical modeling\", \"machine learning\", and even \"Artifical Intelligence\". We've seen various learning systems in previous lectures, and today we're going to examine so-called **recommender system**. In the material above, we talked about the basics of a **content-based** recommendation system. In the case of news, for example, recommended articles could be surfaced if you express interest in one topic or another. Other information about the stories could also be used to produce a ranking -- you might also include the number of \"likes\" or tweets, the publisher, the publication date and so on. \n",
    "\n",
    "This is the first of three approaches Alex Spangher wrote about in his NYT piece on their [next generation news recommender](https://open.blogs.nytimes.com/2015/08/11/building-the-next-new-york-times-recommendation-engine/). The second approach can be implemented without any information about content, and instead relies on how users rate or access content. On Amazon or Yelp or TripAdvisor, we have ratings that people provide for services. At the Times, we don't really have ratings, but instead we have whether or not someone read an article. **Collaborative filtering** uses patterns in these activities to make recommendations. (We \"filter\" items, recommending just a few, and this is a \"collaborative\" act because it relies on the input of others.)\n",
    "\n",
    "At the heart of collaborative filtering is something called a \"utility matrix\" or \"incidence matrix\". It's a table where rows refer to users (shoppers, readers), say, and columns are items (news articles, consumer goods). A recommendation system \"fills in\" or predicts how the user in row i will react to the object represented by column j -- that is, the table entry (i,j). Oh, math!\n",
    "\n",
    "So, here's a cartoon. Let's decide if we are going to recommend a great article on Climate Change to Marie. Marie is represented by row i in the incidence matrix and our Climate Change article is represented by column j. In row i we have 0's and 1's -- a 0 if Marie has not read the article and a 1 if she has. Column j is also filled with 0's and 1's, but this time an entry is 0 if the corresponding reader hasn't selected the article and 1 if he has. Got it? A row refers to the choices by a single user, and a column refers to the choices made by all our users for a single article. \n",
    "\n",
    "Now, we have two possible ways to use our incidence matrix, our table of 0's and 1's, to predict whether Marie will like our Climate Change article. The first is to find readers that are \"similar\" to Marie in terms of reading habits. That is, they have read some of the same articles or have 0's and 1's in the same places across their rows. We can take some number of readers who are most similar to Marie (the number always being called k) and then see what fraction read the article in question. In the image below, the gray band refers to the k readers who are similar to Marie and the red stripe highlights a column representing their reading of the single article. \n",
    "\n",
    "<img src=https://github.com/computationaljournalism/columbia2018/raw/master/images/abcabc.001.jpeg width=700>\n",
    "\n",
    "The second approach to fill in the (i,j) entry of a table like this would be to look at  articles similar to the Climate Change piece we want to recommend to Marie. Articles are similar if they were read (or not) by the same people. We can then take the k most similar articles and see what fraction of them Marie has read. If it's high, we recommend the article to her. The figure below is just the flip of the one we've seen. The grey band is now similar articles to the given one and the red stripe looks at whether or not Marie read these. \n",
    "\n",
    "<img src=https://github.com/computationaljournalism/columbia2018/raw/master/images/abcabc.002.jpeg width=700>\n",
    "\n",
    "This seems pretty straightforward (I hope). The techniques here are referred to ask \"k-nearest neighbors\" (or KNN or kNN). It's actually a pretty powerful machine learning (well, statistical) technique. And we leverage this kind of scheme all the time -- your doctor, for example, distills \"you\" into a row in a table, a height, a weight, a blood pressure, maybe some facts from your medical history. She then gives you advice about dropping a few pounds, say by what the medical profession knows about people like you. The idea is that the health outcomes of people \"similar\" to you can help predict what's around the corner for you. \n",
    "\n",
    "The key ideas here are pretty fundamental and **they have to do with distance.** Rows, for example, can be compared. In the case of collaborative filtering, we can evaluate one reader relative to another, marking some as close (likeminded, similar tastes) and others as far (red state, different tastes). This is a basic, mathematical abstraction that machine learning applies to all tables. Rows are points in a \"high-dimensional space\". (The same is true for columns, of course, except that it's typical to have tables -- spreadsheets-- organized so that rows refer to units of observation and columns refer to their characteristics.)\n",
    "\n",
    "Let's see an incidence matrix from a real application. This was given to us by Alex Spangher and his team at the New York Times. It represents the reading habits of 1,800 visitors to the times. For privacy, we've stripped off anything identifying. But, rows refer to users and columns to URLs. \n",
    "\n",
    "[Download the file from Dropbox and put it in the same folder as your notebook.](https://www.dropbox.com/s/btj4g8z9q4hny7v/incidence.csv?dl=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "incidence = read_csv(\"incidence.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "incidence.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first five rows of the `incidence` data frame shows us whether reader 3, say, read URL1, URL2 and so on. What you notice is that there are a lot of zeroes. Most people don't read most of the New York Times. \n",
    "\n",
    "Let's have a look at how big this table is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "incidence.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, 1,815 users and 10K or so URL's. These represent articles read by the users for a one-month period. (Which, again, I am not at liberty to divulge -- be happy you see a real example of one of these matrices!)\n",
    "\n",
    "Row and column sums can be important. Summing down the columns counts how many people (out of 1,815) read accessed the URL. Here we `apply()` the `sum()` function down columns, indicated with `axis=0`. We sort the resuts and see that many pages have just a single reader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "incidence.apply(sum,axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "incidence.apply(sum,axis=0).sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compute just how many have only one reader. We take the column sum and ask for a boolean outcome, `True` if there was one reader and `False` for more. Adding these up turns `True` to 1 and `False` to zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sum( incidence.apply(sum,axis=0) == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That means 4,221 or about 40% are read just by one person. How many have two? Three? Less than five?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at just the heavy users. These are people who have read more than, say, 25 articles. This requires an `apply()` of `sum()` this time along rows, or `axis=1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "heavy = incidence[incidence.apply(sum,axis=1) >= 25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "heavy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How should we judge similarity between two readers? Again, we want them to have read much the same articles. A simple measure for that is the **Jaccard metric.** It is good for rows (or columns) that are made up of 0's and 1's. Essentially it looks at how many 1's the two readers have in common, divided by the total number of articles they accessed. Well, you subtract that radio from 1. So, if two readers have nothing in common, the metric will be 1, and if they have everything in common, it will be 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Jaccard distance\n",
    "\n",
    "def dist(a,b):\n",
    "    \n",
    "    intersection = sum((a+b)==2)\n",
    "    union = sum((a+b)>=1)\n",
    "    \n",
    "    # print intersection,union\n",
    "    \n",
    "    return 1.0-(intersection)/union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try out the function. We'll make two series (Pandas' representation of a row or column) of 0's and 1's and computes their Jaccard distance. See if this makes sense. Change the 0's and 1's to all agree or disagree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas import Series\n",
    "x = Series([1,0,1,1])\n",
    "y = Series([0,0,1,1])\n",
    "dist(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to compute the distance between two users, we need to access rows. This is done using `.iloc[]` subsetting. We haven't seen this yet, but it gives us fine-grained control over the rows or columns we want to extract. The notation is \n",
    ">`row selection, column selection`\n",
    "\n",
    "Before the comma refers to rows and after to columns. We can use single integers for a single row or column, a slice like `5:10` to get a range,  and the empty slice `:` to ask for all the rows or columns. The result is a Pandas Series. \n",
    "\n",
    "Here is user with ID 40. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "heavy.iloc[40,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This person has read 114 articles in our month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sum(heavy.iloc[40,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is the distance between user 40 and user 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dist(heavy.iloc[40,:],heavy.iloc[10,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loop iterates through the entire set of rows and calculates the distance between user 40 and the rest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(877):\n",
    "    d = dist(heavy.iloc[20,:],heavy.iloc[i,:])\n",
    "    print(i,d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to build on this idea of distance, but use a different incidence matrix. It is perverse but gives you the idea... Now, let's take this notion of distance and push it really, um, far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=https://github.com/computationaljournalism/columbia2018/raw/master/images/recommender-systems-13-638.jpg width=500>\n",
    "    \n",
    "\n",
    "### Collaborative baking\n",
    "\n",
    "So far, this is pretty straightforward (I hope). The techniques here are referred to ask \"k-nearest neighbors\" (or KNN or kNN). It's actually a pretty powerful machine learning (well, statistical) technique. And we leverage this kind of scheme all the time -- your doctor, for example, distills \"you\" into a row in a table, a height, a weight, a blood pressure, maybe some facts from your medical history. She then gives you advice about dropping a few pounds, say by what the medical profession knows about people like you. The idea is that the health outcomes of people \"similar\" to you can help predict what's around the corner for you. \n",
    "\n",
    "The key ideas here are pretty fundamental and **they have to do with distance.** Rows, for example, can be compared. In the case of collaborative filtering, we can evaluate one recipe relative to another, marking some as close (similar ingredients) and others as far (different tastes). This is a basic, mathematical abstraction that machine learning applies to all tables. Rows are points in a \"high-dimensional space\". (The same is true for columns, of course, except that it's typical to have tables -- spreadsheets-- organized so that rows refer to units of observation and columns refer to their characteristics.)\n",
    "\n",
    "Let's see an incidence matrix from our cakes data set. It is weenie, but I'm down the rabbit hole now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "cakes = read_csv(\"https://github.com/computationaljournalism/columbia2018/raw/master/data/cakes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cakes.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first five rows of the `incidence` data frame. Again, the matrix holds a 0 in row i and column j if the recipe in row i is missing ingredient j. It's 1 otherwise. Remember how many cakes we had..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cakes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, 1,000 recipes and 381 ingredients. As we commented in the last notebook, row and column sums can be important. Summing down the columns counts how many recipes (out of 1,000) contain each ingredient. Here we `apply()` the `sum()` function down columns, indicated with `axis=0`. We sort the resuts and see that many ingredients appear just once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cakes.apply(sum,axis=0).sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compute just how many ingredients appear in one recipe. We take the column sum and ask for a boolean outcome, `True` if there was one recipe and `False` for more. Adding these up turns `True` to 1 and `False` to zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum( cakes.apply(sum,axis=0) == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That means 126 or about 13% are used by just one recipe. How many have two? Three? Less than five?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How should we judge similarity between two recipes? Again, we want them to have use much the same ingredients. A simple measure for that is the **Jaccard metric.** It is good for rows (or columns) that are made up of 0's and 1's. Essentially it looks at how many 1's the two recipes have in common, divided by the total number of ingredients they require. Well, you subtract that ratio from 1. So, if two recipes have nothing in common, the metric will be 1, and if they have everything in common, it will be 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Jaccard distance\n",
    "\n",
    "def dist(a,b):\n",
    "    \n",
    "    intersection = sum((a+b)==2)\n",
    "    union = sum((a+b)>=1)\n",
    "    \n",
    "    # print intersection,union\n",
    "    \n",
    "    return 1.0-(0.0+intersection)/union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try out the function. We'll make two series (Pandas' representation of a row or column) of 0's and 1's and computes their Jaccard distance. See if this makes sense. Change the 0's and 1's to all agree or disagree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import Series\n",
    "x = Series([1,0,1,1])\n",
    "y = Series([0,0,1,1])\n",
    "dist(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to compute the distance between two recipes, we need to access rows. This is done using `.iloc[]` subsetting. We haven't seen this yet, but it gives us fine-grained control over the rows or columns we want to extract. The notation is \n",
    ">`row selection, column selection`\n",
    "\n",
    "Before the comma refers to rows and after to columns. We can use single integers for a single row or column, a slice like `5:10` to get a range,  and the empty slice `:` to ask for all the rows or columns. The result is a Pandas Series. \n",
    "\n",
    "Here is recipe with ID 40. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cakes.iloc[40,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This recipe has read 10 ingredients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(cakes.iloc[40,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[c for c in cakes.columns if cakes[c][10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is the distance between recipe 40 and recipe 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[c for c in cakes.columns if cakes[c][40]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two recipes share 8 ingredients and between them, there are 12 total ingredients. That means the distance is 1-8/12 = 1-2/3 = 1/3. Let's see!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist(cakes.iloc[40,:],cakes.iloc[10,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes! \n",
    "\n",
    "Now, this loop iterates through the entire set of rows and calculates the distance between recipe 40 and the rest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = Series([ dist(cakes.iloc[40,:],cakes.iloc[i,:]) for i in range(1000)])\n",
    "distances.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we sort the distaances and look at the distance of the 25th nearest recipe to number 40."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close = distances.sort_values().reset_index(drop=True)[25]\n",
    "close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then subset just the nearby cakes, those with distance less than 0.417. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "near_cakes = cakes[(distances<=close) & (distances>0)]\n",
    "near_cakes.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pull out the ingredients in recipe 40, storing them in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredients = [ing for ing in cakes.columns if cakes[ing][40]]\n",
    "ingredients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we sort the ingredients according to the number of recipes that contain them. We then leave out all the ingredients that are already in cake 40. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = near_cakes.apply(sum,axis=0).sort_values(ascending=False).reset_index()\n",
    "recommendations[~recommendations[\"index\"].isin(ingredients)].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's wrap these two into a function and have a look at a few cakes and what we recommend to add."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(a,b):\n",
    "    \n",
    "    intersection = sum((a+b)==2)\n",
    "    union = sum((a+b)>=1)\n",
    "    \n",
    "    # print intersection,union\n",
    "    \n",
    "    return 1.0-(0.0+intersection)/union\n",
    "\n",
    "def recommend(c,k=10,recipes=cakes):\n",
    "    \n",
    "    n_recipes = recipes.shape[0]\n",
    "    \n",
    "    ingredients = [ing for ing in cakes.columns if cakes[ing][c]]\n",
    "    print(\"Ingredients in recipe\",c)\n",
    "    print(ingredients)\n",
    "    \n",
    "    distances = Series([dist(recipes.iloc[c,:],recipes.iloc[i,:]) for i in range(n_recipes)])\n",
    "    close = list(distances.sort_values())[k]\n",
    "\n",
    "    near_recipes = recipes[(distances<=close) & (distances>0)]\n",
    "    recommendations = near_recipes.apply(sum,axis=0).sort_values(ascending=False).reset_index()\n",
    "    \n",
    "    return recommendations[~recommendations[\"index\"].isin(ingredients)][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommend(920,25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommend(230,25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommend(430,25)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
